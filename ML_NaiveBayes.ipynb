{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCsv(filename):\n",
    "    lines=csv.reader(open(filename, 'r'))\n",
    "    dataset = list(lines)\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = [x for x in dataset[i]]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(numbers):\n",
    "    if len(numbers) == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        avg = mean(numbers)\n",
    "        variance = sum([pow(x-avg,2) for x in numbers]) / float(len(numbers)-1)\n",
    "        return math.sqrt(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(dataset, sRatio):\n",
    "    trainSize = int(len(dataset) * sRatio)\n",
    "    trainSet = []\n",
    "    copy = list(dataset)\n",
    "    while len(trainSet) < trainSize:\n",
    "        index = random.randrange(len(copy))\n",
    "        trainSet.append(copy.pop(index))\n",
    "    return [trainSet, copy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(dataset):\n",
    "    foreveryclass=[]\n",
    "    for attribute in zip(*dataset):\n",
    "        x = mean(attribute)\n",
    "        y = std(attribute)\n",
    "        foreveryclass.append([x,y])\n",
    "    del foreveryclass[-1]\n",
    "    return foreveryclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassData(dataset):\n",
    "    classdivision = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if(vector[-1] not in classdivision):\n",
    "            classdivision[vector[-1]] = []\n",
    "        classdivision[vector[-1]].append(vector)\n",
    "    return classdivision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(dataset):\n",
    "    divided = ClassData(dataset)\n",
    "    PValues = {}\n",
    "    for classValue, instances in divided.items():\n",
    "        PValues[classValue] = process(instances)\n",
    "    return PValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prob(x, mean, stdev):\n",
    "    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "    return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassProb(ProcessValues, inputVector):\n",
    "    probabilities = {}\n",
    "    for classValue, classSummaries in ProcessValues.items():\n",
    "        probabilities[classValue] = 1\n",
    "        for i in range(len(classSummaries)):\n",
    "            mean, stdev = classSummaries[i]\n",
    "            x = inputVector[i]\n",
    "            probabilities[classValue] *=Prob(x,mean,stdev)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ProcessValues , inputVector):\n",
    "    probabilities = ClassProb(ProcessValues, inputVector)\n",
    "    bestLabel , bestProb = None,-1\n",
    "    for classValue , probability in probabilities.items():\n",
    "        if bestLabel is None or probability > bestProb:\n",
    "            bestProb = probability\n",
    "            bestLabel = classValue\n",
    "    return bestLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(ProcessValue , testSet):\n",
    "    predictions = []\n",
    "    y_true = []\n",
    "    for i in range(len(testSet)):\n",
    "        result = predict(ProcessValue , testSet[i])\n",
    "        predictions.append(result)\n",
    "    for i in range(len(testSet)):\n",
    "        vector = testSet[i]\n",
    "        y_true.append(vector[-1])\n",
    "    return [y_true , predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(testSet , predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(testSet)):\n",
    "        if testSet[i][-1] == predictions[i]:\n",
    "            correct +=1\n",
    "    return (correct/float(len(testSet))) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnvrt2flot(dataset):\n",
    "    d=[]\n",
    "    for i in dataset:\n",
    "        d.append(list(map(float,i)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main(dataset):\n",
    "    sRatio = 0.80\n",
    "    #dataset = loadCsv(file)\n",
    "    #dataset = dataset[1:]\n",
    "    #dataset = cnvrt2flot(dataset)\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    training, test = splitData(dataset, sRatio)\n",
    "    redshift = [i[-2] for i in test]\n",
    "    redshift_ranges = [1 if i <= 0.033 else 2 if i >=\n",
    "                       0.004 else 3 for i in redshift]\n",
    "    [i.pop(-2) for i in training]\n",
    "    [i.pop(-2) for i in test]\n",
    "    PV = summary(training)\n",
    "    y_true, predictions = getPredictions(PV, test)\n",
    "    rs = pd.Series(redshift_ranges)\n",
    "    p = pd.Series(predictions)\n",
    "    corrl = rs.corr(p)\n",
    "    cm = confusion_matrix(y_true, predictions)\n",
    "    # print('\\n\\n Confusion Matrix \\n')\n",
    "    # print('\\n'.join([''.join(['{:4}'.format(item) for item in row]) for row in cm]))\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)\n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    # print('False Positives\\n{}'.format(FP))\n",
    "    # print('False Negatives\\n{}'.format(FN))\n",
    "    # print('True Positives\\n{}'.format(TP))\n",
    "    # print('True Negatives\\n{}'.format(TN))\n",
    "    # TPR = TP/(TP+FN)\n",
    "    # print('Sensitivity \\n {}'.format(TPR))\n",
    "    # TNR = TN/(TN+FP)\n",
    "    # print('Specificity \\n {}'.format(TNR))\n",
    "    Precision = TP/(TP+FP)\n",
    "    # print('Precision \\n{}'.format(Precision))\n",
    "    Recall = TP/(TP+FN)\n",
    "    # print('Recall \\n{}'.format(Recall))\n",
    "    Acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "    # print('Accuracy \\n{}'.format(Acc))\n",
    "    Fscore = 2*(Precision*Recall)/(Precision+Recall)\n",
    "    # print('Fscore \\n{}'.format(Fscore))\n",
    "    return Acc,corrl,Precision,Recall,Fscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from mlxtend.evaluate import bias_variance_decomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def mymain(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(labels=['galex_objid', 'sdss_objid'], axis=1, inplace=True)\n",
    "    class_column = df['class']\n",
    "    srs_column = df['spectrometric_redshift']\n",
    "    df = df.drop(labels=['spectrometric_redshift', 'pred', 'class'], axis=1)\n",
    "    df['spectrometric_redshift'] = srs_column\n",
    "    df['class'] = class_column\n",
    "    \n",
    "    df4 = df.drop(labels=['extinction_u','extinction_g','extinction_r','extinction_i','extinction_z'],axis=1)\n",
    "    df2 = df.drop(labels=['nuv-u','nuv-g','nuv-r','nuv-i','nuv-z','u-g','u-r','u-i','u-z','g-r','g-i','g-z','r-i','r-z','i-z'],axis=1)\n",
    "    df3 = df.drop(labels=['fuv-nuv','fuv-u','fuv-g','fuv-r','fuv-i','fuv-z'],axis=1)\n",
    "    df1 = df.drop(labels=['u','g','i','z','extinction_u','extinction_g','extinction_r','extinction_i','extinction_z','nuv-u','nuv-g','nuv-i','nuv-z','u-g','u-r','u-i','u-z','g-r','g-i','g-z','r-i','r-z','i-z','fuv-nuv','fuv-u','fuv-g','fuv-r','fuv-i','fuv-z'],axis=1)\n",
    "    dataset = df.values.tolist()\n",
    "    print('--------------')\n",
    "    print(file)\n",
    "    random.seed(41)\n",
    "    \n",
    "    df_results = []\n",
    "    Acc,corrl,Precision,Recall,Fscore = main(dataset)\n",
    "    Acc1,corrl1,Precision1,Recall1,Fscore1 = main(df1.values.tolist())\n",
    "    Acc2,corrl2,Precision2,Recall2,Fscore2 = main(df2.values.tolist())\n",
    "    Acc3,corrl3,Precision3,Recall3,Fscore3 = main(df3.values.tolist())\n",
    "    Acc4,corrl4,Precision4,Recall4,Fscore4 = main(df4.values.tolist())\n",
    "    data = {'DataSet':['df','df1','df2','df3','df4'], 'Accuracy':[Acc[0],Acc1[0],Acc2[0],Acc3[0],Acc4[0]], 'Correlation':[corrl,corrl1,corrl2,corrl3,corrl4], 'Precision':[Precision[1],Precision1[1],Precision2[1],Precision3[1],Precision4[1]], 'Recall':[Recall[1],Recall1[1],Recall2[1],Recall3[1],Recall4[1]], 'Fscore':[Fscore[1],Fscore1[1],Fscore2[1],Fscore3[1],Fscore4[1]]}\n",
    "    df_results = pd.DataFrame(data)\n",
    "    print(df_results)\n",
    "    \n",
    "    \n",
    "    ## finding the bias_variance decompostion for each combination of datasets in each catalogs\n",
    "    \n",
    "    X = np.array(df.drop('class',axis=1).values)\n",
    "    y = np.array(df['class'].values)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=41,stratify=y)\n",
    "    model = GaussianNB()\n",
    "    model.fit(X,y)\n",
    "\n",
    "    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "            model, X_train, y_train, X_test, y_test, \n",
    "            loss='0-1_loss',\n",
    "            random_seed=41)\n",
    "    avg_loss=avg_expected_loss\n",
    "    avg_bias=avg_bias\n",
    "    avg_var=avg_var\n",
    "    \n",
    "    ## dataset1\n",
    "    X = np.array(df1.drop('class',axis=1).values)\n",
    "    y = np.array(df1['class'].values)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=41,stratify=y)\n",
    "    model = GaussianNB()\n",
    "    model.fit(X,y)\n",
    "\n",
    "    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "            model, X_train, y_train, X_test, y_test, \n",
    "            loss='0-1_loss',\n",
    "            random_seed=41)\n",
    "    avg_loss1=avg_expected_loss\n",
    "    avg_bias1=avg_bias\n",
    "    avg_var1=avg_var\n",
    "    \n",
    "    ##dataset2\n",
    "    X = np.array(df2.drop('class',axis=1).values)\n",
    "    y = np.array(df2['class'].values)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=41,stratify=y)\n",
    "    model = GaussianNB()\n",
    "    model.fit(X,y)\n",
    "\n",
    "    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "            model, X_train, y_train, X_test, y_test, \n",
    "            loss='0-1_loss',\n",
    "            random_seed=41)\n",
    "    avg_loss2=avg_expected_loss\n",
    "    avg_bias2=avg_bias\n",
    "    avg_var2=avg_var\n",
    "    \n",
    "    \n",
    "    ##dataset3\n",
    "    X = np.array(df3.drop('class',axis=1).values)\n",
    "    y = np.array(df3['class'].values)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=41,stratify=y)\n",
    "    model = GaussianNB()\n",
    "    model.fit(X,y)\n",
    "\n",
    "    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "            model, X_train, y_train, X_test, y_test, \n",
    "            loss='0-1_loss',\n",
    "            random_seed=41)\n",
    "    avg_loss3=avg_expected_loss\n",
    "    avg_bias3=avg_bias\n",
    "    avg_var3=avg_var\n",
    "    \n",
    "    \n",
    "    ##datset4\n",
    "    X = np.array(df4.drop('class',axis=1).values)\n",
    "    y = np.array(df4['class'].values)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=41,stratify=y)\n",
    "    model = GaussianNB()\n",
    "    model.fit(X,y)\n",
    "\n",
    "    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "            model, X_train, y_train, X_test, y_test, \n",
    "            loss='0-1_loss',\n",
    "            random_seed=41)\n",
    "    avg_loss4=avg_expected_loss\n",
    "    avg_bias4=avg_bias\n",
    "    avg_var4=avg_var\n",
    "    bv_results = []\n",
    "    data1 = {'DataSet':['df','df1','df2','df3','df4'], 'Avg_exp_loss':[avg_loss,avg_loss1,avg_loss2,avg_loss3,avg_loss4], 'Avg_Bias':[avg_bias,avg_bias1,avg_bias2,avg_bias3,avg_bias4], 'Avg_Variance':[avg_var,avg_var1,avg_var2,avg_var3,avg_var4]}\n",
    "    bv_results = pd.DataFrame(data1)\n",
    "    clf = GaussianNB()\n",
    "    dataset=df1.values.tolist()\n",
    "    X = np.array(df1.drop('class',axis=1).values)\n",
    "    y = np.array(df1['class'].values)\n",
    "    scores = model_selection.cross_val_score(clf, X, y, cv=4, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "    print(file)\n",
    "    print(bv_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import model_selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "cat1.csv\n",
      "  DataSet  Accuracy  Correlation  Precision    Recall    Fscore\n",
      "0      df  0.930769     0.679803   0.965517  0.957265  0.961373\n",
      "1     df1  0.900000     0.379566   0.982609  0.911290  0.945607\n",
      "2     df2  0.892308     0.516984   0.981818  0.900000  0.939130\n",
      "3     df3  0.900000     0.384900   0.957265  0.933333  0.945148\n",
      "4     df4  0.953846     0.661524   1.000000  0.951220  0.975000\n",
      "Accuracy: 1.00 (+/- 0.01)\n",
      "cat1.csv\n",
      "  DataSet  Avg_exp_loss  Avg_Bias  Avg_Variance\n",
      "0      df      0.013744  0.010256      0.004846\n",
      "1     df1      0.006744  0.005128      0.003462\n",
      "2     df2      0.023513  0.020513      0.004590\n",
      "3     df3      0.017513  0.020513      0.006128\n",
      "4     df4      0.011513  0.010256      0.004846\n",
      "--------------\n",
      "cat2.csv\n",
      "  DataSet  Accuracy  Correlation  Precision    Recall    Fscore\n",
      "0      df  0.883562     0.569350   0.963025  0.900943  0.930950\n",
      "1     df1  0.793151     0.302926   0.925347  0.831513  0.875924\n",
      "2     df2  0.853425     0.477573   0.952542  0.876755  0.913079\n",
      "3     df3  0.871233     0.501201   0.965174  0.888550  0.925278\n",
      "4     df4  0.898630     0.594506   0.967105  0.915888  0.940800\n",
      "Accuracy: 0.84 (+/- 0.10)\n",
      "cat2.csv\n",
      "  DataSet  Avg_exp_loss  Avg_Bias  Avg_Variance\n",
      "0      df      0.074630  0.074954      0.004867\n",
      "1     df1      0.102431  0.101463      0.006527\n",
      "2     df2      0.076289  0.074954      0.006929\n",
      "3     df3      0.081444  0.079525      0.008117\n",
      "4     df4      0.074657  0.074954      0.004867\n",
      "--------------\n",
      "cat3.csv\n",
      "  DataSet  Accuracy  Correlation  Precision    Recall    Fscore\n",
      "0      df  0.892899     0.537404   0.961801  0.916775  0.938748\n",
      "1     df1  0.812573     0.333163   0.945428  0.837908  0.888427\n",
      "2     df2  0.857974     0.484093   0.958213  0.877309  0.915978\n",
      "3     df3  0.898719     0.577179   0.965753  0.919166  0.941884\n",
      "4     df4  0.890570     0.568788   0.970588  0.904700  0.936486\n",
      "Accuracy: 0.92 (+/- 0.01)\n",
      "cat3.csv\n",
      "  DataSet  Avg_exp_loss  Avg_Bias  Avg_Variance\n",
      "0      df      0.086656  0.090768      0.010365\n",
      "1     df1      0.078154  0.079131      0.015229\n",
      "2     df2      0.083588  0.085337      0.015613\n",
      "3     df3      0.088479  0.089992      0.010023\n",
      "4     df4      0.087184  0.090768      0.010365\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    files = ['cat1.csv', 'cat2.csv','cat3.csv']\n",
    "    for file in files:\n",
    "        mymain(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
